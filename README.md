# MOSIM LLMs - Pretrain with Chakra Tracing

PyTorch 2.4.0 ê¸°ë°˜ GPT-2 ë° BERT Pretraining í”„ë¡œì íŠ¸ with Chakra Execution Trace ìº¡ì²˜

**SQuAD ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ Data Parallel (DDP) í•™ìŠµ ë° Chakra trace ìº¡ì²˜**

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
mosim-llms/
â”œâ”€â”€ Dockerfile                  # Docker ì´ë¯¸ì§€ ë¹Œë“œ íŒŒì¼
â”œâ”€â”€ docker-compose.yml          # Docker Compose ì„¤ì •
â”œâ”€â”€ build.sh                    # ì´ë¯¸ì§€ ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ run.sh                      # ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ requirements.txt            # Python ì˜ì¡´ì„±
â”‚
â”œâ”€â”€ gpt2/                       # GPT-2 pretrain êµ¬í˜„
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train.py               # GPT-2 í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (DDP ì§€ì›)
â”‚   â”œâ”€â”€ model.py               # GPT-2 ëª¨ë¸ ì •ì˜
â”‚   â””â”€â”€ config.py              # GPT-2 ì„¤ì •
â”‚
â”œâ”€â”€ bert/                       # BERT pretrain êµ¬í˜„
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train.py               # BERT í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (DDP ì§€ì›)
â”‚   â”œâ”€â”€ model.py               # BERT ëª¨ë¸ ì •ì˜
â”‚   â””â”€â”€ config.py              # BERT ì„¤ì •
â”‚
â”œâ”€â”€ utils/                      # ê³µí†µ ìœ í‹¸ë¦¬í‹°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ distributed.py         # DDP ì„¤ì • ìœ í‹¸
â”‚   â”œâ”€â”€ data_utils.py          # SQuAD ë°ì´í„° ë¡œë”©
â”‚   â””â”€â”€ chakra_tracer.py       # Chakra trace ìº¡ì²˜
â”‚
â”œâ”€â”€ run_gpt2_quick.sh           # GPT-2 í”„ë¡œíŒŒì¼ë§ (1/2/4 GPU)
â”œâ”€â”€ run_bert_quick.sh           # BERT í”„ë¡œíŒŒì¼ë§ (1/2/4 GPU)
â”œâ”€â”€ run_all_profiling.sh        # ì „ì²´ í”„ë¡œíŒŒì¼ë§ ìë™ ì‹¤í–‰
â”‚
â”œâ”€â”€ Chakra ET ë³€í™˜ ë„êµ¬ (ë¡œì»¬ìš©)
â”œâ”€â”€ setup_and_convert.sh        # ìë™ ì„¤ì¹˜ + ë³€í™˜
â”œâ”€â”€ fix_host_traces.py          # Host trace JSON ìˆ˜ì •
â”œâ”€â”€ convert_to_et.py            # Chakra ET ë³€í™˜
â”œâ”€â”€ convert_remaining.py        # ì‹¤íŒ¨í•œ íŒŒì¼ ì¬ë³€í™˜
â”œâ”€â”€ install_chakra_tools.sh     # Chakra ë„êµ¬ ìˆ˜ë™ ì„¤ì¹˜
â”‚
â”œâ”€â”€ ë¬¸ì„œ
â”œâ”€â”€ README.md                   # í”„ë¡œì íŠ¸ ê°€ì´ë“œ
â”œâ”€â”€ USAGE_GUIDE.md              # ìƒì„¸ ì‚¬ìš©ë²•
â”œâ”€â”€ PROJECT_SUMMARY.md          # í”„ë¡œì íŠ¸ ìš”ì•½
â”œâ”€â”€ CHAKRA_SETUP.md             # Chakra ì„¤ì • ê°€ì´ë“œ
â””â”€â”€ CONVERSION_SUMMARY.md       # ë³€í™˜ ê²°ê³¼ ìš”ì•½
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### A. ì„œë²„ì—ì„œ í”„ë¡œíŒŒì¼ë§ ì‹¤í–‰

ì„œë²„(GPU í™˜ê²½)ì—ì„œ PyTorch í”„ë¡œíŒŒì¼ë§ì„ ìˆ˜í–‰í•˜ì—¬ trace íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.

#### 1. ì½”ë“œ ì—…ë°ì´íŠ¸ ë° Docker ì´ë¯¸ì§€ ë¹Œë“œ

```bash
# ìµœì‹  ì½”ë“œ ê°€ì ¸ì˜¤ê¸°
sudo git pull

# Docker ì´ë¯¸ì§€ ë¹Œë“œ
sudo ./build.sh
```

#### 2. Docker ì»¨í…Œì´ë„ˆ ì‹¤í–‰

```bash
# GPU ì»¨í…Œì´ë„ˆ ì‹¤í–‰
./run.sh gpu

# ë˜ëŠ” docker-compose ì‚¬ìš©
docker-compose up -d
docker-compose exec mosim-llms bash
```

#### 3. í”„ë¡œíŒŒì¼ë§ ì‹¤í–‰ (ì»¨í…Œì´ë„ˆ ë‚´ë¶€)

```bash
# ëª¨ë“  í”„ë¡œíŒŒì¼ë§ ìë™ ì‹¤í–‰ (GPT-2, BERT Ã— 1/2/4 GPU)
./run_all_profiling.sh

# ë˜ëŠ” ê°œë³„ ì‹¤í–‰
./run_gpt2_quick.sh    # GPT-2: 1/2/4 GPU
./run_bert_quick.sh    # BERT: 1/2/4 GPU
```

**ìƒì„±ë˜ëŠ” íŒŒì¼** (outputs í´ë”):
```
outputs/
â”œâ”€â”€ bert_1gpu_quick_trace_host.json      # Host trace
â”œâ”€â”€ bert_1gpu_quick_trace_device.json    # Device trace
â”œâ”€â”€ bert_1gpu_quick_trace_stacks.txt     # ë¶„ì„ìš©
â”œâ”€â”€ bert_2gpu_quick_trace_*.json
â”œâ”€â”€ bert_4gpu_quick_trace_*.json
â”œâ”€â”€ gpt2_1gpu_quick_trace_*.json
â”œâ”€â”€ gpt2_2gpu_quick_trace_*.json
â””â”€â”€ gpt2_4gpu_quick_trace_*.json
```

### B. ë¡œì»¬(ë§¥ë¶)ì—ì„œ Chakra ET ë³€í™˜

ì„œë²„ì—ì„œ ìƒì„±ëœ trace íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ Chakra ET í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

#### 1. ì„œë²„ì—ì„œ trace íŒŒì¼ ë‹¤ìš´ë¡œë“œ

```bash
# ë¡œì»¬ ë§¥ë¶ì—ì„œ ì‹¤í–‰
scp -r your-server:/path/to/mosim-llms/outputs ./
```

#### 2. Chakra ë„êµ¬ ìë™ ì„¤ì¹˜ ë° ë³€í™˜

```bash
# í•œ ë²ˆì— ì„¤ì¹˜ + ë³€í™˜ (ê¶Œì¥)
./setup_and_convert.sh
```

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ìë™ìœ¼ë¡œ:
- âœ… Python ê°€ìƒ í™˜ê²½ ìƒì„± (`chakra-venv/`)
- âœ… Chakra ë„êµ¬ ì„¤ì¹˜ (PARAM, HolisticTraceAnalysis, Chakra)
- âœ… Host trace JSON ìˆ˜ì • (ì¤‘ë³µ ê°ì²´ ì œê±°)
- âœ… Host + Device trace ë³‘í•©
- âœ… Chakra ET íŒŒì¼ ìƒì„±

#### 3. ë³€í™˜ ê²°ê³¼ í™•ì¸

```bash
# ET íŒŒì¼ í™•ì¸
ls -lh outputs/*.et

# ì˜ˆìƒ ì¶œë ¥:
# bert_1gpu_quick_trace.et      (10 MB)
# bert_2gpu_quick_trace.et      (12 MB)
# bert_4gpu_quick_trace.et      (12 MB)
# gpt2_1gpu_quick_trace.et      (8.6 MB)
# gpt2_2gpu_quick_trace.et      (9.6 MB)
# gpt2_4gpu_quick_trace.et      (9.6 MB)
```

#### ğŸ“ ìˆ˜ë™ ë³€í™˜ (ë¬¸ì œ ë°œìƒ ì‹œ)

ìë™ ë³€í™˜ì´ ì‹¤íŒ¨í•œ ê²½ìš°:

```bash
# 1. ê°€ìƒ í™˜ê²½ í™œì„±í™”
source chakra-venv/bin/activate

# 2. Host trace ìˆ˜ì • (ì¤‘ë³µ JSON ê°ì²´ ì œê±°)
python fix_host_traces.py

# 3. ET ë³€í™˜
python convert_to_et.py

# 4. ê°€ìƒ í™˜ê²½ ë¹„í™œì„±í™”
deactivate
```

**ë³€í™˜ ê³¼ì •:**
1. `fix_host_traces.py` - ì†ìƒëœ host trace JSON ìˆ˜ì •
2. `chakra_trace_link` - host + device trace ë³‘í•©
3. `chakra_converter` - Chakra ET í˜•ì‹ìœ¼ë¡œ ë³€í™˜

**ë¬¸ì œ í•´ê²°:**
- ìì„¸í•œ ê°€ì´ë“œ: [CHAKRA_SETUP.md](CHAKRA_SETUP.md)
- ë³€í™˜ ê²°ê³¼: [CONVERSION_SUMMARY.md](CONVERSION_SUMMARY.md)

## ìƒì„¸ ì‚¬ìš©ë²•

### GPT-2 í•™ìŠµ

```bash
# ë‹¨ì¼ GPU
python gpt2/train.py \
    --model-name gpt2 \
    --batch-size 4 \
    --num-epochs 3 \
    --num-gpus 1 \
    --enable-tracing \
    --trace-output-dir ./outputs \
    --trace-name gpt2_1gpu

# ë©€í‹° GPU (DDP)
torchrun --nproc_per_node=8 gpt2/train.py \
    --model-name gpt2 \
    --batch-size 4 \
    --num-epochs 3 \
    --num-gpus 8 \
    --enable-tracing \
    --trace-output-dir ./outputs \
    --trace-name gpt2_8gpu
```

### BERT í•™ìŠµ

```bash
# ë‹¨ì¼ GPU
python bert/train.py \
    --model-name bert-base-uncased \
    --batch-size 4 \
    --num-epochs 3 \
    --num-gpus 1 \
    --enable-tracing

# ë©€í‹° GPU (DDP)
torchrun --nproc_per_node=8 bert/train.py \
    --model-name bert-base-uncased \
    --batch-size 4 \
    --num-epochs 3 \
    --num-gpus 8 \
    --enable-tracing
```

## Data Parallelism êµ¬í˜„

- **ë°©ì‹**: PyTorch DistributedDataParallel (DDP)
- **FSDP ë¯¸ì‚¬ìš©**: TensorFlowì˜ `MultiWorkerMirroredStrategy`ì™€ ìœ ì‚¬í•œ ì „í†µì ì¸ DP ë°©ì‹
- **ì§€ì› GPU ìˆ˜**: 1, 2, 8
- **Batch Size**: GPUë‹¹ 4 (ì¡°ì • ê°€ëŠ¥)

### DDP íŠ¹ì§•
- ê° GPUê°€ ëª¨ë¸ ì „ì²´ ë³µì‚¬ë³¸ì„ ê°€ì§
- Gradient ë™ê¸°í™”ë¥¼ í†µí•œ í•™ìŠµ
- `torch.nn.parallel.DistributedDataParallel` ì‚¬ìš©
- `torchrun`ì„ í†µí•œ ë©€í‹°í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰

## ğŸ“Š Chakra Execution Trace

### Trace ì›Œí¬í”Œë¡œìš°

**ì„œë²„ â†’ ë¡œì»¬ 2ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤:**

#### 1ë‹¨ê³„: ì„œë²„ì—ì„œ PyTorch Profiling
```
PyTorch Profiler
â”œâ”€â”€ ExecutionTraceObserver â†’ *_host.json    (CPU operations)
â””â”€â”€ Kineto Profiler â†’ *_device.json         (GPU operations)
```

#### 2ë‹¨ê³„: ë¡œì»¬ì—ì„œ Chakra ET ë³€í™˜
```
Local Conversion
â”œâ”€â”€ fix_host_traces.py â†’ host.json ìˆ˜ì •
â”œâ”€â”€ chakra_trace_link â†’ host + device ë³‘í•©
â””â”€â”€ chakra_converter â†’ .et íŒŒì¼ ìƒì„±
```

### ìƒì„±ë˜ëŠ” íŒŒì¼

#### ì„œë²„ (PyTorch Profiling)
```
outputs/
â”œâ”€â”€ *_host.json        # ExecutionTraceObserver ì¶œë ¥
â”œâ”€â”€ *_device.json      # Kineto Profiler ì¶œë ¥
â””â”€â”€ *_stacks.txt       # Stack trace ë¶„ì„
```

#### ë¡œì»¬ (Chakra ë³€í™˜ í›„)
```
outputs/
â”œâ”€â”€ *_host.json        # ì›ë³¸ (ìˆ˜ì •ë¨)
â”œâ”€â”€ *_device.json      # ì›ë³¸
â”œâ”€â”€ *_merged.json      # ë³‘í•©ëœ trace (ì¤‘ê°„ íŒŒì¼)
â””â”€â”€ *.et              # Chakra ET (ìµœì¢… ì¶œë ¥) âœ“
```

### ìµœì¢… ET íŒŒì¼

| ëª¨ë¸ | GPU | íŒŒì¼ëª… | í¬ê¸° |
|------|-----|--------|------|
| BERT | 1 | `bert_1gpu_quick_trace.et` | 10 MB |
| BERT | 2 | `bert_2gpu_quick_trace.et` | 12 MB |
| BERT | 4 | `bert_4gpu_quick_trace.et` | 12 MB |
| GPT-2 | 1 | `gpt2_1gpu_quick_trace.et` | 8.6 MB |
| GPT-2 | 2 | `gpt2_2gpu_quick_trace.et` | 9.6 MB |
| GPT-2 | 4 | `gpt2_4gpu_quick_trace.et` | 9.6 MB |

**ìº¡ì²˜ ë‚´ìš©:**
- âœ… Compute operations (forward, backward)
- âœ… Memory operations (allocation, transfer)
- âœ… Communication operations (all-reduce, broadcast for DDP)
- âœ… Dependency graph
- âœ… Timing information

### Chakra ë„êµ¬ ì„¤ì¹˜

#### ì„œë²„ (Docker)
Docker ì´ë¯¸ì§€ì— **ì´ë¯¸ í¬í•¨**ë˜ì–´ ìˆìŠµë‹ˆë‹¤:
- âœ… PARAM (et_replay)
- âœ… HolisticTraceAnalysis (chakra_trace_link)
- âœ… Chakra (chakra_converter)

ë³„ë„ ì„¤ì¹˜ ë¶ˆí•„ìš”!

#### ë¡œì»¬ (ë§¥ë¶)
`setup_and_convert.sh` ìŠ¤í¬ë¦½íŠ¸ê°€ **ìë™ ì„¤ì¹˜**í•©ë‹ˆë‹¤:

```bash
./setup_and_convert.sh
# ìë™ìœ¼ë¡œ ì„¤ì¹˜:
# - Python ê°€ìƒ í™˜ê²½ (chakra-venv/)
# - PARAM, HolisticTraceAnalysis, Chakra
```

**ìˆ˜ë™ ì„¤ì¹˜**ê°€ í•„ìš”í•œ ê²½ìš°:
```bash
./install_chakra_tools.sh
# ë˜ëŠ”
source chakra-venv/bin/activate
pip install git+https://github.com/mlcommons/chakra.git
```

ìì„¸í•œ ì„¤ì¹˜ ê°€ì´ë“œ: [CHAKRA_SETUP.md](CHAKRA_SETUP.md)

## ë°ì´í„°ì…‹

### SQuAD (Stanford Question Answering Dataset)

- **GPT-2**: Causal Language Modeling
  - Question + Contextë¥¼ concatenateí•˜ì—¬ í•™ìŠµ
  - Next token prediction

- **BERT**: Masked Language Modeling
  - `[CLS] question [SEP] context [SEP]` í˜•ì‹
  - 15% token masking

## Docker ì´ë¯¸ì§€

### ë² ì´ìŠ¤ ì´ë¯¸ì§€
- **ì´ë¯¸ì§€**: `pytorch/pytorch:2.4.0-cuda12.4-cudnn9-runtime`
- **PyTorch**: 2.4.0
- **CUDA**: 12.4
- **cuDNN**: 9

### í¬í•¨ëœ ë„êµ¬
- PyTorch 2.4.0
- Transformers (HuggingFace)
- Datasets (HuggingFace)
- Chakra (MLCommons)
- TensorBoard
- Weights & Biases

### ë³¼ë¥¨ ë§ˆìš´íŠ¸
- `./outputs` â†’ `/workspace/mosim-llms/outputs`
- `./data` â†’ `/workspace/mosim-llms/data`
- `./checkpoints` â†’ `/workspace/mosim-llms/checkpoints`

## ì¶œë ¥ íŒŒì¼

### ì²´í¬í¬ì¸íŠ¸
```
checkpoints/
â”œâ”€â”€ gpt2/
â”‚   â””â”€â”€ best_model/
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ pytorch_model.bin
â”‚       â””â”€â”€ ...
â””â”€â”€ bert/
    â””â”€â”€ best_model/
        â”œâ”€â”€ config.json
        â”œâ”€â”€ pytorch_model.bin
        â””â”€â”€ ...
```

### Chakra Traces
```
outputs/
â”œâ”€â”€ *_kineto.json          # PyTorch Kineto trace (ì¤‘ê°„ í˜•ì‹)
â”œâ”€â”€ *_stacks.txt           # Stack trace analysis
â””â”€â”€ *.et                   # Chakra ET format (ìµœì¢… ì¶œë ¥) âœ“
```

## ì‹¤í—˜ ì„¤ì •

| Model | Dataset | GPUs | Batch Size/GPU | Effective Batch | DP Method |
|-------|---------|------|----------------|-----------------|-----------|
| GPT-2 | SQuAD   | 1    | 4              | 4               | N/A       |
| GPT-2 | SQuAD   | 2    | 4              | 8               | DDP       |
| GPT-2 | SQuAD   | 8    | 4              | 32              | DDP       |
| BERT  | SQuAD   | 1    | 4              | 4               | N/A       |
| BERT  | SQuAD   | 2    | 4              | 8               | DDP       |
| BERT  | SQuAD   | 8    | 4              | 32              | DDP       |

## ë¼ì´ì„ ìŠ¤

Apache-2.0 License (Chakraì™€ ë™ì¼)

## ğŸ”§ Docker ì´ë¯¸ì§€ ìµœì í™”

### ì‚¬ì „ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ & í† í¬ë‚˜ì´ì €

Docker ì´ë¯¸ì§€ì—ëŠ” ë‹¤ìŒì´ **ë¯¸ë¦¬ í¬í•¨**ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

- âœ… GPT-2 tokenizer (`/workspace/pretrained_models/gpt2`)
- âœ… BERT tokenizer (`/workspace/pretrained_models/bert-base-uncased`)

**ì¥ì :**
- ì¸í„°ë„· ì—°ê²° ì—†ì´ ì‹¤í–‰ ê°€ëŠ¥
- ë§¤ë²ˆ ë‹¤ìš´ë¡œë“œí•  í•„ìš” ì—†ìŒ
- HuggingFace Hub ì ‘ê·¼ ë¬¸ì œ í•´ê²°

**ìë™ í´ë°±:** ì´ë¯¸ì§€ ë‚´ í† í¬ë‚˜ì´ì €ê°€ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
