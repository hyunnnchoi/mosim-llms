# MOSIM LLMs - Pretrain with Chakra Tracing

PyTorch 2.4.0 ê¸°ë°˜ GPT-2 ë° BERT Pretraining í”„ë¡œì íŠ¸ with Chakra Execution Trace ìº¡ì²˜

**SQuAD ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ Data Parallel (DDP) í•™ìŠµ ë° Chakra trace ìº¡ì²˜**

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
mosim-llms/
â”œâ”€â”€ Dockerfile                  # Docker ì´ë¯¸ì§€ ë¹Œë“œ íŒŒì¼
â”œâ”€â”€ docker-compose.yml          # Docker Compose ì„¤ì •
â”œâ”€â”€ build.sh                    # ì´ë¯¸ì§€ ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ run.sh                      # ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ requirements.txt            # Python ì˜ì¡´ì„±
â”‚
â”œâ”€â”€ gpt2/                       # GPT-2 pretrain êµ¬í˜„
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train.py               # GPT-2 í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (DDP ì§€ì›)
â”‚   â”œâ”€â”€ model.py               # GPT-2 ëª¨ë¸ ì •ì˜
â”‚   â””â”€â”€ config.py              # GPT-2 ì„¤ì •
â”‚
â”œâ”€â”€ bert/                       # BERT pretrain êµ¬í˜„
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train.py               # BERT í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (DDP ì§€ì›)
â”‚   â”œâ”€â”€ model.py               # BERT ëª¨ë¸ ì •ì˜
â”‚   â””â”€â”€ config.py              # BERT ì„¤ì •
â”‚
â”œâ”€â”€ utils/                      # ê³µí†µ ìœ í‹¸ë¦¬í‹°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ distributed.py         # DDP ì„¤ì • ìœ í‹¸
â”‚   â”œâ”€â”€ data_utils.py          # SQuAD ë°ì´í„° ë¡œë”©
â”‚   â””â”€â”€ chakra_tracer.py       # Chakra trace ìº¡ì²˜
â”‚
â”œâ”€â”€ run_gpt2.sh                 # GPT-2 ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ run_bert.sh                 # BERT ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ run_all_experiments.sh      # ì „ì²´ ì‹¤í—˜ ìë™ ì‹¤í–‰
```

## ë¹ ë¥¸ ì‹œì‘

### 1. Docker ì´ë¯¸ì§€ ë¹Œë“œ

```bash
./build.sh
```

### 2. ì»¨í…Œì´ë„ˆ ì‹¤í–‰

```bash
# GPU ì‚¬ìš©
./run.sh gpu

# ë˜ëŠ” docker-compose ì‚¬ìš©
docker-compose up -d
docker-compose exec mosim-llms bash
```

### 3. í•™ìŠµ ì‹¤í–‰

ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ:

```bash
# GPT-2: 1 GPU
./run_gpt2.sh 1

# GPT-2: 2 GPUs
./run_gpt2.sh 2

# GPT-2: 8 GPUs
./run_gpt2.sh 8

# BERT: 1 GPU
./run_bert.sh 1

# BERT: 2 GPUs
./run_bert.sh 2

# BERT: 8 GPUs
./run_bert.sh 8

# ëª¨ë“  ì‹¤í—˜ ìë™ ì‹¤í–‰
./run_all_experiments.sh
```

## ìƒì„¸ ì‚¬ìš©ë²•

### GPT-2 í•™ìŠµ

```bash
# ë‹¨ì¼ GPU
python gpt2/train.py \
    --model-name gpt2 \
    --batch-size 4 \
    --num-epochs 3 \
    --num-gpus 1 \
    --enable-tracing \
    --trace-output-dir ./outputs \
    --trace-name gpt2_1gpu

# ë©€í‹° GPU (DDP)
torchrun --nproc_per_node=8 gpt2/train.py \
    --model-name gpt2 \
    --batch-size 4 \
    --num-epochs 3 \
    --num-gpus 8 \
    --enable-tracing \
    --trace-output-dir ./outputs \
    --trace-name gpt2_8gpu
```

### BERT í•™ìŠµ

```bash
# ë‹¨ì¼ GPU
python bert/train.py \
    --model-name bert-base-uncased \
    --batch-size 4 \
    --num-epochs 3 \
    --num-gpus 1 \
    --enable-tracing

# ë©€í‹° GPU (DDP)
torchrun --nproc_per_node=8 bert/train.py \
    --model-name bert-base-uncased \
    --batch-size 4 \
    --num-epochs 3 \
    --num-gpus 8 \
    --enable-tracing
```

## Data Parallelism êµ¬í˜„

- **ë°©ì‹**: PyTorch DistributedDataParallel (DDP)
- **FSDP ë¯¸ì‚¬ìš©**: TensorFlowì˜ `MultiWorkerMirroredStrategy`ì™€ ìœ ì‚¬í•œ ì „í†µì ì¸ DP ë°©ì‹
- **ì§€ì› GPU ìˆ˜**: 1, 2, 8
- **Batch Size**: GPUë‹¹ 4 (ì¡°ì • ê°€ëŠ¥)

### DDP íŠ¹ì§•
- ê° GPUê°€ ëª¨ë¸ ì „ì²´ ë³µì‚¬ë³¸ì„ ê°€ì§
- Gradient ë™ê¸°í™”ë¥¼ í†µí•œ í•™ìŠµ
- `torch.nn.parallel.DistributedDataParallel` ì‚¬ìš©
- `torchrun`ì„ í†µí•œ ë©€í‹°í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰

## Chakra Execution Trace

### Trace ìº¡ì²˜

í•™ìŠµ ì¤‘ ìë™ìœ¼ë¡œ PyTorch Kineto traceë¥¼ ìƒì„±í•˜ê³  Chakra ET íŒŒì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤:

- **ì¤‘ê°„ í˜•ì‹**: Kineto trace JSON (PyTorch profiler ì¶œë ¥)
- **ìµœì¢… í˜•ì‹**: Chakra Execution Trace (`.et`)
- **ì €ì¥ ìœ„ì¹˜**: `./outputs/`
- **ìº¡ì²˜ ë‚´ìš©**:
  - Compute operations (forward, backward)
  - Memory operations (allocation, transfer)
  - Communication operations (all-reduce for DDP)
  - Timing information
  - Stack traces

### Trace íŒŒì¼

```
outputs/
â”œâ”€â”€ gpt2_1gpu_trace_kineto.json      # PyTorch Kineto trace
â”œâ”€â”€ gpt2_1gpu_trace.et               # Chakra ET íŒŒì¼ âœ“
â”œâ”€â”€ gpt2_1gpu_trace_stacks.txt       # ë¶„ì„ìš© í…ìŠ¤íŠ¸
â”œâ”€â”€ gpt2_2gpu_trace_kineto.json
â”œâ”€â”€ gpt2_2gpu_trace.et               # Chakra ET íŒŒì¼ âœ“
â”œâ”€â”€ gpt2_8gpu_trace_kineto.json
â”œâ”€â”€ gpt2_8gpu_trace.et               # Chakra ET íŒŒì¼ âœ“
â”œâ”€â”€ bert_1gpu_trace_kineto.json
â”œâ”€â”€ bert_1gpu_trace.et               # Chakra ET íŒŒì¼ âœ“
â”œâ”€â”€ bert_2gpu_trace_kineto.json
â”œâ”€â”€ bert_2gpu_trace.et               # Chakra ET íŒŒì¼ âœ“
â”œâ”€â”€ bert_8gpu_trace_kineto.json
â””â”€â”€ bert_8gpu_trace.et               # Chakra ET íŒŒì¼ âœ“
```

### Chakra ET ë³€í™˜

ìë™ ë³€í™˜ì´ ì‹¤íŒ¨í•˜ëŠ” ê²½ìš° ìˆ˜ë™ìœ¼ë¡œ ë³€í™˜ ê°€ëŠ¥:

```bash
# Chakra Python API ì‚¬ìš©
python -m chakra.et_converter.pytorch \
    --input outputs/gpt2_1gpu_trace_kineto.json \
    --output outputs/gpt2_1gpu_trace.et

# ë˜ëŠ” ì§ì ‘ Python ìŠ¤í¬ë¦½íŠ¸
from chakra.et_converter.pytorch import PyTorchConverter
converter = PyTorchConverter()
converter.convert("input.json", "output.et")
```

## ë°ì´í„°ì…‹

### SQuAD (Stanford Question Answering Dataset)

- **GPT-2**: Causal Language Modeling
  - Question + Contextë¥¼ concatenateí•˜ì—¬ í•™ìŠµ
  - Next token prediction

- **BERT**: Masked Language Modeling
  - `[CLS] question [SEP] context [SEP]` í˜•ì‹
  - 15% token masking

## Docker ì´ë¯¸ì§€

### ë² ì´ìŠ¤ ì´ë¯¸ì§€
- **ì´ë¯¸ì§€**: `pytorch/pytorch:2.4.0-cuda12.4-cudnn9-runtime`
- **PyTorch**: 2.4.0
- **CUDA**: 12.4
- **cuDNN**: 9

### í¬í•¨ëœ ë„êµ¬
- PyTorch 2.4.0
- Transformers (HuggingFace)
- Datasets (HuggingFace)
- Chakra (MLCommons)
- TensorBoard
- Weights & Biases

### ë³¼ë¥¨ ë§ˆìš´íŠ¸
- `./outputs` â†’ `/workspace/mosim-llms/outputs`
- `./data` â†’ `/workspace/mosim-llms/data`
- `./checkpoints` â†’ `/workspace/mosim-llms/checkpoints`

## ì¶œë ¥ íŒŒì¼

### ì²´í¬í¬ì¸íŠ¸
```
checkpoints/
â”œâ”€â”€ gpt2/
â”‚   â””â”€â”€ best_model/
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ pytorch_model.bin
â”‚       â””â”€â”€ ...
â””â”€â”€ bert/
    â””â”€â”€ best_model/
        â”œâ”€â”€ config.json
        â”œâ”€â”€ pytorch_model.bin
        â””â”€â”€ ...
```

### Chakra Traces
```
outputs/
â”œâ”€â”€ *_kineto.json          # PyTorch Kineto trace (ì¤‘ê°„ í˜•ì‹)
â”œâ”€â”€ *_stacks.txt           # Stack trace analysis
â””â”€â”€ *.et                   # Chakra ET format (ìµœì¢… ì¶œë ¥) âœ“
```

## ì‹¤í—˜ ì„¤ì •

| Model | Dataset | GPUs | Batch Size/GPU | Effective Batch | DP Method |
|-------|---------|------|----------------|-----------------|-----------|
| GPT-2 | SQuAD   | 1    | 4              | 4               | N/A       |
| GPT-2 | SQuAD   | 2    | 4              | 8               | DDP       |
| GPT-2 | SQuAD   | 8    | 4              | 32              | DDP       |
| BERT  | SQuAD   | 1    | 4              | 4               | N/A       |
| BERT  | SQuAD   | 2    | 4              | 8               | DDP       |
| BERT  | SQuAD   | 8    | 4              | 32              | DDP       |

## ë¼ì´ì„ ìŠ¤

Apache-2.0 License (Chakraì™€ ë™ì¼)

## ğŸ”§ Docker ì´ë¯¸ì§€ ìµœì í™”

### ì‚¬ì „ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ & í† í¬ë‚˜ì´ì €

Docker ì´ë¯¸ì§€ì—ëŠ” ë‹¤ìŒì´ **ë¯¸ë¦¬ í¬í•¨**ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

- âœ… GPT-2 tokenizer (`/workspace/pretrained_models/gpt2`)
- âœ… BERT tokenizer (`/workspace/pretrained_models/bert-base-uncased`)

**ì¥ì :**
- ì¸í„°ë„· ì—°ê²° ì—†ì´ ì‹¤í–‰ ê°€ëŠ¥
- ë§¤ë²ˆ ë‹¤ìš´ë¡œë“œí•  í•„ìš” ì—†ìŒ
- HuggingFace Hub ì ‘ê·¼ ë¬¸ì œ í•´ê²°

**ìë™ í´ë°±:** ì´ë¯¸ì§€ ë‚´ í† í¬ë‚˜ì´ì €ê°€ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
