# MOSIM LLMs - Pretrain with Chakra Tracing

PyTorch 2.4.0 ê¸°ë°˜ GPT-2 ë° BERT Pretraining í”„ë¡œì íŠ¸ with Chakra Execution Trace ìº¡ì²˜

**SQuAD ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ Data Parallel (DDP) í•™ìŠµ ë° Chakra trace ìº¡ì²˜**

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
mosim-llms/
â”œâ”€â”€ Dockerfile                  # Docker ì´ë¯¸ì§€ ë¹Œë“œ íŒŒì¼
â”œâ”€â”€ docker-compose.yml          # Docker Compose ì„¤ì •
â”œâ”€â”€ build.sh                    # ì´ë¯¸ì§€ ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ run.sh                      # ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ requirements.txt            # Python ì˜ì¡´ì„±
â”‚
â”œâ”€â”€ gpt2/                       # GPT-2 pretrain êµ¬í˜„
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train.py               # GPT-2 í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (DDP ì§€ì›)
â”‚   â”œâ”€â”€ model.py               # GPT-2 ëª¨ë¸ ì •ì˜
â”‚   â””â”€â”€ config.py              # GPT-2 ì„¤ì •
â”‚
â”œâ”€â”€ bert/                       # BERT pretrain êµ¬í˜„
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train.py               # BERT í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (DDP ì§€ì›)
â”‚   â”œâ”€â”€ model.py               # BERT ëª¨ë¸ ì •ì˜
â”‚   â””â”€â”€ config.py              # BERT ì„¤ì •
â”‚
â”œâ”€â”€ utils/                      # ê³µí†µ ìœ í‹¸ë¦¬í‹°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ distributed.py         # DDP ì„¤ì • ìœ í‹¸
â”‚   â”œâ”€â”€ data_utils.py          # SQuAD ë°ì´í„° ë¡œë”©
â”‚   â””â”€â”€ chakra_tracer.py       # Chakra trace ìº¡ì²˜
â”‚
â”œâ”€â”€ run_gpt2.sh                 # GPT-2 ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ run_bert.sh                 # BERT ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ run_all_experiments.sh      # ì „ì²´ ì‹¤í—˜ ìë™ ì‹¤í–‰
```

## ë¹ ë¥¸ ì‹œì‘

### 1. Docker ì´ë¯¸ì§€ ë¹Œë“œ

```bash
./build.sh
```

### 2. ì»¨í…Œì´ë„ˆ ì‹¤í–‰

```bash
# GPU ì‚¬ìš©
./run.sh gpu

# ë˜ëŠ” docker-compose ì‚¬ìš©
docker-compose up -d
docker-compose exec mosim-llms bash
```

### 3. í•™ìŠµ ì‹¤í–‰

ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ:

```bash
# GPT-2: 1 GPU
./run_gpt2.sh 1

# GPT-2: 2 GPUs
./run_gpt2.sh 2

# GPT-2: 8 GPUs
./run_gpt2.sh 8

# BERT: 1 GPU
./run_bert.sh 1

# BERT: 2 GPUs
./run_bert.sh 2

# BERT: 8 GPUs
./run_bert.sh 8

# ëª¨ë“  ì‹¤í—˜ ìë™ ì‹¤í–‰
./run_all_experiments.sh
```

## ìƒì„¸ ì‚¬ìš©ë²•

### GPT-2 í•™ìŠµ

```bash
# ë‹¨ì¼ GPU
python gpt2/train.py \
    --model-name gpt2 \
    --batch-size 4 \
    --num-epochs 3 \
    --num-gpus 1 \
    --enable-tracing \
    --trace-output-dir ./outputs \
    --trace-name gpt2_1gpu

# ë©€í‹° GPU (DDP)
torchrun --nproc_per_node=8 gpt2/train.py \
    --model-name gpt2 \
    --batch-size 4 \
    --num-epochs 3 \
    --num-gpus 8 \
    --enable-tracing \
    --trace-output-dir ./outputs \
    --trace-name gpt2_8gpu
```

### BERT í•™ìŠµ

```bash
# ë‹¨ì¼ GPU
python bert/train.py \
    --model-name bert-base-uncased \
    --batch-size 4 \
    --num-epochs 3 \
    --num-gpus 1 \
    --enable-tracing

# ë©€í‹° GPU (DDP)
torchrun --nproc_per_node=8 bert/train.py \
    --model-name bert-base-uncased \
    --batch-size 4 \
    --num-epochs 3 \
    --num-gpus 8 \
    --enable-tracing
```

## Data Parallelism êµ¬í˜„

- **ë°©ì‹**: PyTorch DistributedDataParallel (DDP)
- **FSDP ë¯¸ì‚¬ìš©**: TensorFlowì˜ `MultiWorkerMirroredStrategy`ì™€ ìœ ì‚¬í•œ ì „í†µì ì¸ DP ë°©ì‹
- **ì§€ì› GPU ìˆ˜**: 1, 2, 8
- **Batch Size**: GPUë‹¹ 4 (ì¡°ì • ê°€ëŠ¥)

### DDP íŠ¹ì§•
- ê° GPUê°€ ëª¨ë¸ ì „ì²´ ë³µì‚¬ë³¸ì„ ê°€ì§
- Gradient ë™ê¸°í™”ë¥¼ í†µí•œ í•™ìŠµ
- `torch.nn.parallel.DistributedDataParallel` ì‚¬ìš©
- `torchrun`ì„ í†µí•œ ë©€í‹°í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰

## Chakra Execution Trace

### Trace ìº¡ì²˜ ì›Œí¬í”Œë¡œìš°

í•™ìŠµ ì¤‘ ìë™ìœ¼ë¡œ PyTorch traceë¥¼ ìƒì„±í•˜ê³  Chakra ET íŒŒì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤:

**ë³€í™˜ íŒŒì´í”„ë¼ì¸:**
1. **PyTorch Profiler** â†’ Kineto trace JSON (Chrome trace)
2. **chakra_trace_link** â†’ Host + Device merge
3. **chakra_converter** â†’ Chakra ET (protobuf)

- **ì¤‘ê°„ í˜•ì‹**: 
  - `*_kineto.json`: PyTorch Kineto trace (Chrome JSON)
  - `*_chakra_host_device.json`: Merged trace (Chakra ì…ë ¥)
- **ìµœì¢… í˜•ì‹**: Chakra Execution Trace (`.et`)
- **ì €ì¥ ìœ„ì¹˜**: `./outputs/`
- **ìº¡ì²˜ ë‚´ìš©**:
  - Compute operations (forward, backward)
  - Memory operations (allocation, transfer)
  - Communication operations (all-reduce for DDP)
  - Dependency graph
  - Timing information

### Trace íŒŒì¼

```
outputs/
â”œâ”€â”€ gpt2_1gpu_quick_trace_kineto.json           # Step 1: Kineto trace
â”œâ”€â”€ gpt2_1gpu_quick_trace_chakra_host_device.json  # Step 2: Merged
â”œâ”€â”€ gpt2_1gpu_quick_trace.et                    # Step 3: Chakra ET âœ“
â”œâ”€â”€ gpt2_1gpu_quick_trace_stacks.txt            # ë¶„ì„ìš©
â”œâ”€â”€ gpt2_2gpu_quick_trace.et                    # Chakra ET âœ“
â”œâ”€â”€ gpt2_4gpu_quick_trace.et                    # Chakra ET âœ“
â”œâ”€â”€ bert_1gpu_quick_trace.et                    # Chakra ET âœ“
â”œâ”€â”€ bert_2gpu_quick_trace.et                    # Chakra ET âœ“
â””â”€â”€ bert_4gpu_quick_trace.et                    # Chakra ET âœ“
```

### Chakra ì˜ì¡´ì„± ì„¤ì¹˜

Dockerfileì— í¬í•¨ëœ ì˜ì¡´ì„±:
- **PARAM** (Chakra í•„ìˆ˜ ì˜ì¡´ì„±)
- **HolisticTraceAnalysis** (chakra_trace_link ì œê³µ)
- **Chakra** (ë³€í™˜ ë„êµ¬)

ìˆ˜ë™ ì„¤ì¹˜ (ì„œë²„ì—ì„œ):
```bash
# PARAM ì„¤ì¹˜ (et_replay)
git clone https://github.com/facebookresearch/param.git
cd param/et_replay
git checkout 7b19f586dd8b267333114992833a0d7e0d601630
pip install .

# HolisticTraceAnalysis ì„¤ì¹˜
git clone https://github.com/facebookresearch/HolisticTraceAnalysis.git
cd HolisticTraceAnalysis
git checkout d731cc2e2249976c97129d409a83bd53d93051f6
git submodule update --init
pip install -r requirements.txt
pip install -e .

# Chakra ì¬ì„¤ì¹˜
pip install git+https://github.com/mlcommons/chakra.git
```

### Chakra ET ë³€í™˜ (ìˆ˜ë™)

ìë™ ë³€í™˜ì´ ì‹¤íŒ¨í•˜ëŠ” ê²½ìš°:

```bash
# Step 1: chakra_trace_linkë¡œ host + device merge
chakra_trace_link \
    --chakra-host-trace outputs/gpt2_1gpu_quick_trace_kineto.json \
    --chakra-device-trace outputs/gpt2_1gpu_quick_trace_kineto.json \
    --rank 0 \
    --output-file outputs/gpt2_1gpu_quick_trace_chakra_host_device.json

# Step 2: chakra_converterë¡œ .et ë³€í™˜
chakra_converter PyTorch \
    --input outputs/gpt2_1gpu_quick_trace_chakra_host_device.json \
    --output outputs/gpt2_1gpu_quick_trace
```

## ë°ì´í„°ì…‹

### SQuAD (Stanford Question Answering Dataset)

- **GPT-2**: Causal Language Modeling
  - Question + Contextë¥¼ concatenateí•˜ì—¬ í•™ìŠµ
  - Next token prediction

- **BERT**: Masked Language Modeling
  - `[CLS] question [SEP] context [SEP]` í˜•ì‹
  - 15% token masking

## Docker ì´ë¯¸ì§€

### ë² ì´ìŠ¤ ì´ë¯¸ì§€
- **ì´ë¯¸ì§€**: `pytorch/pytorch:2.4.0-cuda12.4-cudnn9-runtime`
- **PyTorch**: 2.4.0
- **CUDA**: 12.4
- **cuDNN**: 9

### í¬í•¨ëœ ë„êµ¬
- PyTorch 2.4.0
- Transformers (HuggingFace)
- Datasets (HuggingFace)
- Chakra (MLCommons)
- TensorBoard
- Weights & Biases

### ë³¼ë¥¨ ë§ˆìš´íŠ¸
- `./outputs` â†’ `/workspace/mosim-llms/outputs`
- `./data` â†’ `/workspace/mosim-llms/data`
- `./checkpoints` â†’ `/workspace/mosim-llms/checkpoints`

## ì¶œë ¥ íŒŒì¼

### ì²´í¬í¬ì¸íŠ¸
```
checkpoints/
â”œâ”€â”€ gpt2/
â”‚   â””â”€â”€ best_model/
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ pytorch_model.bin
â”‚       â””â”€â”€ ...
â””â”€â”€ bert/
    â””â”€â”€ best_model/
        â”œâ”€â”€ config.json
        â”œâ”€â”€ pytorch_model.bin
        â””â”€â”€ ...
```

### Chakra Traces
```
outputs/
â”œâ”€â”€ *_kineto.json          # PyTorch Kineto trace (ì¤‘ê°„ í˜•ì‹)
â”œâ”€â”€ *_stacks.txt           # Stack trace analysis
â””â”€â”€ *.et                   # Chakra ET format (ìµœì¢… ì¶œë ¥) âœ“
```

## ì‹¤í—˜ ì„¤ì •

| Model | Dataset | GPUs | Batch Size/GPU | Effective Batch | DP Method |
|-------|---------|------|----------------|-----------------|-----------|
| GPT-2 | SQuAD   | 1    | 4              | 4               | N/A       |
| GPT-2 | SQuAD   | 2    | 4              | 8               | DDP       |
| GPT-2 | SQuAD   | 8    | 4              | 32              | DDP       |
| BERT  | SQuAD   | 1    | 4              | 4               | N/A       |
| BERT  | SQuAD   | 2    | 4              | 8               | DDP       |
| BERT  | SQuAD   | 8    | 4              | 32              | DDP       |

## ë¼ì´ì„ ìŠ¤

Apache-2.0 License (Chakraì™€ ë™ì¼)

## ğŸ”§ Docker ì´ë¯¸ì§€ ìµœì í™”

### ì‚¬ì „ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ & í† í¬ë‚˜ì´ì €

Docker ì´ë¯¸ì§€ì—ëŠ” ë‹¤ìŒì´ **ë¯¸ë¦¬ í¬í•¨**ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

- âœ… GPT-2 tokenizer (`/workspace/pretrained_models/gpt2`)
- âœ… BERT tokenizer (`/workspace/pretrained_models/bert-base-uncased`)

**ì¥ì :**
- ì¸í„°ë„· ì—°ê²° ì—†ì´ ì‹¤í–‰ ê°€ëŠ¥
- ë§¤ë²ˆ ë‹¤ìš´ë¡œë“œí•  í•„ìš” ì—†ìŒ
- HuggingFace Hub ì ‘ê·¼ ë¬¸ì œ í•´ê²°

**ìë™ í´ë°±:** ì´ë¯¸ì§€ ë‚´ í† í¬ë‚˜ì´ì €ê°€ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
